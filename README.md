# â˜ï¸ Cloud ETL Pipeline â€” Azure Databricks + Data Factory + Terraform + CI/CD

A fully automated, production-ready **Cloud ETL pipeline** built with a modern, in-demand Azure data engineering stack.  
This project demonstrates **infrastructure-as-code, data orchestration, and continuous delivery** for scalable and secure ETL workflows.

---

## ğŸš€ Architecture Overview

### High-Level Flow
```
Data Sources â†’ Ingestion (ADF / Auto Loader)
â†’ Transformation (Databricks + Delta Lake)
â†’ Storage (ADLS + Unity Catalog)
â†’ Consumption (Power BI / ML)
```

### Core Components
| Layer | Technology | Purpose |
|--------|-------------|----------|
| **Ingestion** | Azure Data Factory (ADF) | Orchestrate data extraction from APIs, databases, and files |
| **Transformation** | Azure Databricks (Spark + Delta Lake) | Clean, standardize, and enrich data using PySpark |
| **Storage** | ADLS Gen2 + Unity Catalog | Secure, hierarchical data storage and governance |
| **Orchestration** | ADF + GitHub Actions | Automate and schedule ETL jobs |
| **Infrastructure** | Terraform | Provision resources across environments (dev/test/prod) |
| **Data Quality** | Great Expectations | Enforce schema and value-level data validation |
| **Monitoring** | Azure Monitor + Log Analytics | Track pipeline runs, job metrics, and alerts |

---

## ğŸ§± Project Structure

```
cloud-etl/
â”œâ”€ infra/                     # Terraform IaC
â”‚  â”œâ”€ main.tf                 # Providers and Resource Groups
â”‚  â”œâ”€ datalake.tf             # ADLS Gen2 + containers
â”‚  â”œâ”€ databricks.tf           # Workspace + cluster setup
â”‚  â”œâ”€ datafactory.tf          # Pipelines and Linked Services
â”‚  â”œâ”€ keyvault.tf             # Secrets and credentials
â”‚  â”œâ”€ monitor.tf              # Log Analytics and Alerts
â”‚  â””â”€ env/dev.tfvars          # Environment variables
â”‚
â”œâ”€ etl/
â”‚  â”œâ”€ bronze/ingest_autoloader.py    # Ingest raw data using Spark Auto Loader
â”‚  â”œâ”€ silver/clean_and_conform.py    # Clean, validate, and standardize data
â”‚  â”œâ”€ gold/business_curations.py     # Aggregation and business-ready datasets
â”‚  â”œâ”€ common/io_utils.py             # Utility functions for reads/writes
â”‚  â””â”€ common/expectations/           # Great Expectations data validation
â”‚
â”œâ”€ adf/
â”‚  â”œâ”€ pipelines/ingest_pipeline.json # ADF JSON definition
â”‚  â””â”€ datasets_linkedservices/       # Source/Target connection definitions
â”‚
â”œâ”€ tests/
â”‚  â”œâ”€ unit/test_io_utils.py          # Unit tests for helper functions
â”‚  â””â”€ dq/test_expectations.py        # Data quality tests
â”‚
â”œâ”€ .github/workflows/
â”‚  â”œâ”€ ci.yml                         # Linting + Unit Tests on PRs
â”‚  â””â”€ cd-infra-and-jobs.yml          # Terraform Deploy + Databricks sync
â”‚
â”œâ”€ .pre-commit-config.yaml
â”œâ”€ Makefile
â””â”€ README.md
```

---

## ğŸ”§ Setup Instructions

### 1ï¸âƒ£ Prerequisites
- Azure Subscription (Contributor or Owner role)
- Service Principal with:
  - `AZURE_CLIENT_ID`, `AZURE_CLIENT_SECRET`, `AZURE_TENANT_ID`, `AZURE_SUBSCRIPTION_ID`
- Installed locally:
  - `terraform`, `az`, `python3`, `databricks-cli`, `git`

---

### 2ï¸âƒ£ Clone the Repo
```bash
git clone https://github.com/<yourusername>/cloud-etl.git
cd cloud-etl
```

---

### 3ï¸âƒ£ Initialize Terraform
```bash
cd infra
terraform init
terraform workspace new dev || terraform workspace select dev
terraform apply -var-file=env/dev.tfvars
```

This provisions:
- Resource Group  
- ADLS Gen2 (raw, bronze, silver, gold)  
- Databricks Workspace + Cluster  
- Azure Data Factory  
- Key Vault + Log Analytics  

---

### 4ï¸âƒ£ Configure Databricks CLI
```bash
databricks configure --host https://<databricks-instance> --token <PAT_TOKEN>
```

---

### 5ï¸âƒ£ Run ETL Jobs
Use the ADF pipeline trigger or Databricks notebook jobs directly.

Example (manual test):
```bash
databricks workspace import_dir etl /Shared/etl --overwrite
databricks jobs run-now --job-id <your-job-id>
```

---

### 6ï¸âƒ£ Verify Delta Tables
```sql
-- Inside Databricks SQL or notebook
SELECT * FROM main.bronze.sales_raw LIMIT 10;
SELECT * FROM main.silver.sales LIMIT 10;
SELECT * FROM main.gold.daily_sales ORDER BY d DESC;
```

---

## ğŸ” CI/CD Workflow

**GitHub Actions** automates:
- `ci.yml`: Linting, unit tests, and data quality validation on every pull request.  
- `cd-infra-and-jobs.yml`: Applies Terraform infra and syncs Databricks jobs to prod when merged into `main`.

Secrets required in GitHub repo:
- `AZURE_TENANT_ID`, `AZURE_CLIENT_ID`, `AZURE_SUBSCRIPTION_ID`, `AZURE_CLIENT_SECRET`
- `DATABRICKS_HOST`, `DATABRICKS_TOKEN`

---

## ğŸ” Monitoring & Alerts

- Azure Monitor & Log Analytics collect Databricks job and ADF pipeline logs.
- Alerts are configured for:
  - Job or pipeline failures
  - Cost overages
  - Key Vault access anomalies

---

## ğŸ›¡ï¸ Security & Governance

- All secrets are stored in **Azure Key Vault**.  
- **Unity Catalog** manages fine-grained access control across bronze, silver, and gold tables.  
- **RBAC** + **Private Endpoints** ensure secure data flow.

---

## ğŸŒŸ Features

âœ… Cloud-native, serverless ETL  
âœ… Delta Lake with schema evolution  
âœ… Great Expectations-based data validation  
âœ… Infrastructure-as-Code (Terraform)  
âœ… Automated CI/CD with GitHub Actions  
âœ… Monitored and governed with Azure tools  

---

## ğŸ“ˆ Future Enhancements

- Add streaming ingestion (Event Hub â†’ Delta Live Tables)
- Implement MLflow tracking for data drift
- Deploy using Azure DevOps pipelines as an alternative CI/CD
- Add dbt for SQL-based transformation in Gold layer

---

## ğŸ¤ Contributing

1. Create a new feature branch:
   ```bash
   git checkout -b feature/<feature-name>
   ```
2. Commit and push changes:
   ```bash
   git commit -m "Add: <change description>"
   git push origin feature/<feature-name>
   ```
3. Submit a Pull Request for review.

---

## ğŸ“œ License

This project is released under the **MIT License**.

---

**Author:** Amit kumar
**Email:** amitkrg83@gmail.com 
**LinkedIn:** [linkedin.com/in/yourprofile]([https://www.linkedin.com/in/amitkrg83/)
